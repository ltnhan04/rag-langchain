{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc2fc17",
   "metadata": {},
   "source": [
    "1. Load from PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801a4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/2312.16862.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf55c00",
   "metadata": {},
   "source": [
    "1.1. Load per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29c44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='TinyGPT-V: Efficient Multimodal Large Language Model\n",
      "via Small Backbones\n",
      "Zhengqing Yuan1 Zhaoxu Li 2 Weiran Huang3 Yanfang Ye1 Lichao Sun 4\n",
      "Abstract\n",
      "In recent years, multimodal large language mod-\n",
      "els (MLLMs) such as GPT-4V have demonstrated\n",
      "remarkable advancements, excelling in a variety\n",
      "of vision-language tasks. Despite their prowess,\n",
      "the closed-source nature and computational de-\n",
      "mands of such models limit their accessibility and\n",
      "applicability. This study introduces TinyGPT-V ,\n",
      "a novel open-source MLLM, designed for effi-\n",
      "cient training and inference across various vision-\n",
      "language tasks, including image captioning (IC)\n",
      "and visual question answering (VQA). Leverag-\n",
      "ing a compact yet powerful architecture, TinyGPT-\n",
      "V integrates the Phi-2 language model with pre-\n",
      "trained vision encoders, utilizing a unique map-\n",
      "ping module for visual and linguistic information\n",
      "fusion. With a training regimen optimized for\n",
      "small backbones and employing a diverse dataset\n",
      "amalgam, TinyGPT-V requires significantly lower\n",
      "computational resources—24GB for training and\n",
      "as little as 8GB for inference—without compro-\n",
      "mising on performance. Our experiments demon-\n",
      "strate that TinyGPT-V , with its language model\n",
      "2.8 billion parameters, achieves comparable re-\n",
      "sults in VQA and image inference tasks to its\n",
      "larger counterparts while being uniquely suited\n",
      "for deployment on resource-constrained devices\n",
      "through innovative quantization techniques. This\n",
      "work not only paves the way for more accessible\n",
      "and efficient MLLMs but also underscores the po-\n",
      "tential of smaller, optimized models in bridging\n",
      "the gap between high performance and computa-\n",
      "tional efficiency in real-world applications. Addi-\n",
      "tionally, this paper introduces a new approach to\n",
      "multimodal large language models using smaller\n",
      "backbones. Our code and training weights are\n",
      "available in the supplementary material.\n",
      "1Universiy of Notre Dame 2Nanyang Technological University\n",
      "3Shanghai Jiao Tong University 4Lehigh University. Correspon-\n",
      "dence to: Lichao Sun <lis221@lehigh.edu>.\n",
      "Accepted to the Workshop on Advancing Neural Network Training\n",
      "at International Conference on Machine Learning (W ANT@ICML\n",
      "2024).\n",
      "5.07.510.012.515.0\n",
      "Parameter (Billions)\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46Performance Flamingo\n",
      "BLIP-2\n",
      "LLaVA\n",
      "InstructBLIP\n",
      "MiniGPT-4\n",
      "TinyGPT-V\n",
      "Figure 1: Comparison of TinyGPT-V with other current\n",
      "MLLMs models shows TinyGPT-V achieves cost-effective,\n",
      "efficient, and high-performing with fewer parameters.\n",
      "1. Introduction\n",
      "In recent years, the field of artificial intelligence has seen\n",
      "significant advancements through the development of multi-\n",
      "modal large language models (MLLMs), such as GPT-4V ,\n",
      "which have shown exceptional performance across a range\n",
      "of vision-language tasks (Yang et al., 2023). Despite GPT-\n",
      "4V’s impressive capabilities, its closed-source nature limits\n",
      "its widespread application and adaptability. In contrast, the\n",
      "open-source landscape for MLLMs is rapidly evolving, pre-\n",
      "senting models like LLaV A and MiniGPT-4 that excel in\n",
      "image captioning (IC), visual question answering (VQA)\n",
      "often comparable GPT-4V in these areas (Dai et al., 2023;\n",
      "Liu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\n",
      "v2 (Chen et al., 2023) has demonstrated superior perfor-\n",
      "mance in various visual grounding and question-answering\n",
      "tasks. However, its training code remains proprietary, which\n",
      "poses challenges for community-driven advancements and\n",
      "adaptability.\n",
      "Although the impressive vision-language capabilities\n",
      "demonstrated by some open-source MLLMs, they fre-\n",
      "quently necessitate significant computational resources for\n",
      "training and inference. For example, training LLaV A-v1.5-\n",
      "13B (Liu et al., 2023a) required 8 × A100 GPUs, each\n",
      "equipped with 80GB of memory, cumulating in 25.5 hours\n",
      "of continuous training. As shown in Figure 2 (a), the un-\n",
      "1\n",
      "arXiv:2312.16862v3  [cs.CV]  21 Jun 2024' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-24T00:23:57+00:00', 'author': 'Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, Lichao Sun', 'keywords': 'Efficient Deep Learning, WANT, ICML2024', 'moddate': '2024-06-24T00:23:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'TinyGPT-V: Efficient Multimodal Large Language Model  via Small Backbones', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(url)\n",
    "docs = pdf_loader.load()\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a958f6b",
   "metadata": {},
   "source": [
    "1.2. Load pdf from dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba664b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft® Word 2010 Trial', 'creator': 'Microsoft® Word 2010 Trial', 'creationdate': '2025-06-06T15:21:03+07:00', 'author': 'Administrator', 'moddate': '2025-06-06T15:21:03+07:00', 'source': 'data_source/2.+Exploringthepotential.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='HUFLIT Journal of Science \\nEXPLORING THE POTENTIAL OF GRAPH NEURAL NETWORKS FOR \\nVIETNAMESE SENTIMENT ANALYSIS \\nDinh Minh Hoa, Nguyen Tran Thanh Nha, Nguyen Van Xanh, Bui Thi Thanh Tu, Tran Khai Thien* \\n Ho Chi Minh City University of Foreign Languages and Information Technology, Vietnam   \\nhoadm@huflit.edu.vn, 22DH112486@st.huflit.edu.vn, xannv@huflit.edu.vn, tubtt@huflit.edu.vn, \\nthientk@huflit.edu.vn \\n \\nABSTRACT — Graph Neural Networks (GNNs) have demonstrated outstanding potential in various natural language \\nprocessing (NLP) tasks; however, their application to Vietnamese sentiment analysis remains relatively underexplored. This \\nstudy evaluates the effectiveness of several GNN -based models – including TextGCN, HGAT, BertGCN, and GraphSAGE  – in \\nanalyzing sentiments in Vietnamese textual data. Experiments were conducted on two benchmark Vietnamese sentiment \\ndatasets, UIT-VSFC and Foody. The empirical results compare the performance of GNN models with both traditional machine \\nlearning approaches and representative deep learning architectures. Key performance metrics such as accuracy and F1 -\\nscore were analyzed to highlight the strengths of each method. The findings reveal that GNN -based models exhibit su perior \\ncapabilities in capturing contextual and semantic relationships within texts, particularly in complex sentiment scenarios. \\nThis study aims to investigate the potential of applying GNNs to enhance Vietnamese sentiment analysis, offering a novel \\nperspective compared to traditional and deep learning models. Additionally, the implementation code  for the GNN models \\nhas been made available on GitHub† to serve as a resource for other research groups interested in this domain. \\nKeywords — Natural Language Pro cessing, Sentiment Analysis, Vietnamese Sentiment Analysis, Graph Neural Networks, \\nGraph-based Feature Extraction. \\nI. INTRODUCTION \\nSentiment analysis, also known as opinion mining, is a critical task in natural language processing (NLP). It \\ninvolves the automatic identification and classification of opinions, attitudes, and emotions expressed in textual \\ndata. With the explosive growth of user -generated content on social media platforms, e -commerce websites, and \\nonline forums, sentiment analysis has become  an indispensable tool for businesses and organizations. Notable \\napplications of sentiment analysis include monitoring customer satisfaction  [1], predicting market trends  [2], \\nand detecting public sentiment toward political or social issues  [3]. Despite being a well -researched problem, \\nsentiment analysis still presents considerable challenges  – especially for languages with complex linguistic \\nstructures such as Vietnamese. \\nIn recent years, the emergence of GNNs has revolutionized the way structured data is processed in machine \\nlearning. Unlike traditional deep learning models that primarily operate on sequential or tabular data, GNNs \\nexcel at modeling relational data and capturing interdependencies among elements in a graph. These \\ncapabilities make GNNs particularly powerful for NLP tasks, where contextual relationships and syntactic \\nstructures are crucial. By leveraging graph -based representations such as  dependency trees or word co -\\noccurrence graphs, GNNs have shown significant improvements in tasks like machine translation, text \\nclassification, and question answering. \\nIn the context of Vietnamese, applying GNNs to sentiment analysis remains in its early stages. Current \\napproaches to Vietnamese sentiment analysis largely rely on traditional machine-learning models or \\ntransformer-based architectures such as BERT  [4]. While these methods have achieved certain levels of \\neffectiveness, they often struggle to represen t the intricate grammatical and semantic relationships in \\nVietnamese texts. This study focuses on exploring the potential of GNN -based models to address these \\nlimitations and provides a comparative analysis with traditional approaches to highlight the stre ngths of GNNs \\nin Vietnamese sentiment analysis. \\nII. RELATED WORK \\nAs a critical task within NLP, sentiment analysis has significantly expanded its scope of applications alongside \\nthe rapid increase in user -generated content on digital platforms. In the business  domain, it enables companies \\nto monitor customer feedback  [1], enhance service quality, and develop targeted marketing strategies  [5]. \\nPoliticians and policymakers employ sentiment analysis to gauge public opinion, understand socia l trends, and \\nforecast election outcomes. In market research, it assists in evaluating product performance, predicting \\nconsumer demand, and understanding customer behavior. Furthermore, sentiment analysis has found \\n                                                           \\n* Coressponding Author \\n† https://github.com/hoadm-net/TextGraph \\nRESEARCH ARTICLE')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "path_dir = \"./data_source\"\n",
    "pdf_loader = PyPDFDirectoryLoader(path_dir)\n",
    "docs = pdf_loader.load()\n",
    "len(docs)\n",
    "docs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
